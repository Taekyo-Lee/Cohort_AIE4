{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa_QpI0RXQKx"
      },
      "source": [
        "# LangSmith and Evaluation Overview with AI Makerspace\n",
        "\n",
        "Today we'll be looking at an amazing tool:\n",
        "\n",
        "[LangSmith](https://docs.smith.langchain.com/)!\n",
        "\n",
        "This tool will help us monitor, test, debug, and evaluate our LangChain applications - and more!\n",
        "\n",
        "We'll also be looking at a few Advanced Retrieval techniques along the way - and evaluate it using LangSmith!\n",
        "\n",
        "‚úãBREAKOUT ROOM #2:\n",
        "- Task 1: Dependencies and OpenAI API Key\n",
        "- Task 2: LCEL RAG Chain\n",
        "- Task 3: Setting Up LangSmith\n",
        "- Task 4: Examining the Trace in LangSmith!\n",
        "- Task 5: Create Testing Dataset\n",
        "- Task 6: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw5ok9p-XuUs"
      },
      "source": [
        "## Task 1: Dependencies and OpenAI API Key\n",
        "\n",
        "We'll be using OpenAI's suite of models today to help us generate and embed our documents for a simple RAG system built on top of LangChain's blogs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è Like the previous notebook, run some pre prerequisite cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhSjB1O6-Y0J",
        "outputId": "1f43fdba-f1b4-4cf4-b8b0-3469edc29c80"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_core langchain_openai langchain_community langchain-qdrant qdrant-client langsmith openai tiktoken cohere lxml -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADl8-whIAUHD",
        "outputId": "927fd78b-8510-4bea-9e68-a3c74d3eb5a1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923xinz42sWV"
      },
      "source": [
        "#### Asyncio Bug Handling\n",
        "\n",
        "This is necessary for Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "m9U0SbQN2sWc"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx2AOb-QHwJm"
      },
      "source": [
        "## Task #2: Create a Simple RAG Application Using Qdrant, Hugging Face, and LCEL\n",
        "\n",
        "Now that we have a grasp on how LCEL works, and how we can use LangChain and Hugging Face to interact with our data - let's step it up a notch and incorporate Qdrant!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYJB3IxEwpnh"
      },
      "source": [
        "## LangChain Powered RAG\n",
        "\n",
        "First and foremost, LangChain provides a convenient way to store our chunks and their embeddings.\n",
        "\n",
        "It's called a `VectorStore`!\n",
        "\n",
        "We'll be using QDrant as our `VectorStore` today. You can read more about it [here](https://qdrant.tech/documentation/).\n",
        "\n",
        "Think of a `VectorStore` as a smart way to house your chunks and their associated embedding vectors. The implementation of the `VectorStore` also allows for smarter and more efficient search of our embedding vectors - as the method we used above would not scale well as we got into the millions of chunks.\n",
        "\n",
        "Otherwise, the process remains relatively similar under the hood!\n",
        "\n",
        "We'll use a SiteMapLoader to scrape the LangChain blogs - which will serve as our data for today!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBErqPRgxgZR"
      },
      "source": [
        "### Data Collection\n",
        "\n",
        "We'll be leveraging the `SitemapLoader` to load our PDF directly from the web!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è In this notebook, our source of RAG chian is Langchain bolg post."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHA9L3Jxo3r",
        "outputId": "c3535941-ce13-4ddd-ef08-b6b4ab1e507b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching pages: 100%|##########| 220/220 [00:05<00:00, 38.47it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import SitemapLoader\n",
        "\n",
        "documents = SitemapLoader(web_path=\"https://blog.langchain.dev/sitemap-posts.xml\").load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH7ZPVJLx6Cn"
      },
      "source": [
        "### Chunking Our Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSCRQUSyBKT"
      },
      "source": [
        "Let's do the same process as we did before with our `RecursiveCharacterTextSplitter` - but this time we'll use ~200 tokens as our max chunk size!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SzolG1FLx2f_"
      },
      "outputs": [],
      "source": [
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# üòÄ To make it easier to remember, I will import the classes this way\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpV4f1eXyXVJ",
        "outputId": "a666d551-6b20-4761-82ad-6f9b524d1660"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4821"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTJ60Ck6ybe_"
      },
      "source": [
        "Alright, now we have 516 ~200 token long documents.\n",
        "\n",
        "Let's verify the process worked as intended by checking our max document length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "950mB338yZR8",
        "outputId": "9813bfb9-cf80-4787-c962-69a5b1ccdff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "499\n"
          ]
        }
      ],
      "source": [
        "max_chunk_length = 0\n",
        "\n",
        "for chunk in split_chunks:\n",
        "  max_chunk_length = max(max_chunk_length, len(chunk.page_content))\n",
        "\n",
        "print(max_chunk_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDt3RufQy1cP"
      },
      "source": [
        "Perfect! Now we can carry on to creating and storing our embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kocCe4zLy5qT"
      },
      "source": [
        "### Embeddings and Vector Storage\n",
        "\n",
        "We'll use the `text-embedding-3-small` embedding model again - and `Qdrant` to store all our embedding vectors for easy retrieval later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7M0X1eVlWPFf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jetlee/anaconda3/envs/AIE4/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è instead of manually creating a  retrieber, we use vector store and its associated retriever.\n",
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è vector store is a langchain abstraction of vertor database. Many 3rd party vector stores are out there. \n",
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è We select Qdrant vector database\n",
        "\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    documents=split_chunks,\n",
        "    embedding=embedding_model,\n",
        "    location=\":memory:\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-NDvjfzXhVp"
      },
      "source": [
        "Now let's set up our retriever, just as we saw before, but this time using LangChain's simple `as_retriever()` method!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Edjx19YBXavZ"
      },
      "outputs": [],
      "source": [
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è One of the advantages of Langchain voector store is that we can easily get retriever from as_retriever method.\n",
        "\n",
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<class 'str'>\n",
            "typing.List[langchain_core.documents.base.Document]\n"
          ]
        }
      ],
      "source": [
        "# üòÄ My own cell\n",
        "\n",
        "from langchain_core.runnables import Runnable\n",
        "\n",
        "print(isinstance(qdrant_vectorstore, Runnable))\n",
        "print(isinstance(qdrant_retriever, Runnable))\n",
        "print(qdrant_retriever.InputType)\n",
        "print(qdrant_retriever.OutputType)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1OM0DiYcOj-"
      },
      "source": [
        "#### Back to the Flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7apXaEBzQai"
      },
      "source": [
        "We're ready to move to the next step!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMhcU37dzV5k"
      },
      "source": [
        "### Setting up our RAG\n",
        "\n",
        "We'll use the LCEL we touched on earlier to create a RAG chain.\n",
        "\n",
        "Let's think through each part:\n",
        "\n",
        "1. First we need to retrieve context\n",
        "2. We need to pipe that context to our model\n",
        "3. We need to parse that output\n",
        "\n",
        "Let's start by setting up our prompt again, just so it's fresh in our minds!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oatgDa7cOXWV"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Complete the prompt so that your RAG application answers queries based on the context provided, but *does not* answer queries if the context is unrelated to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "TE5tick_YPJj"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# üòÄ I completed the prompt template.\n",
        "base_rag_prompt_template = \"\"\"\\\n",
        "Answer the user's Question based on the given Context. \\\n",
        "If the given Context is not relevant to the Question, please respond with \"I don't know\". Do not make up.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "base_rag_prompt = ChatPromptTemplate.from_template(base_rag_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2tNXIT1iuB"
      },
      "source": [
        "We'll set our Generator - `gpt-4o` in this case - below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "rZ-9gF1x1iEz"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "base_llm = ChatOpenAI(model=\"gpt-4o-mini\", tags=[\"base_llm\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZKadufhc2RL"
      },
      "source": [
        "#### Our RAG Chain\n",
        "\n",
        "Notice how we have a bit of a more complex chain this time - that's because we want to return our sources with the response.\n",
        "\n",
        "Let's break down the chain step-by-step:\n",
        "\n",
        "1. We invoke the chain with the `question` item. Notice how we only need to provide `question` since both the retreiver and the `\"question\"` object depend on it.\n",
        "  - We also chain our `\"question\"` into our `retriever`! This is what ultimately collects the context through Qdrant.\n",
        "2. We assign our collected context to a `RunnablePassthrough()` from the previous object. This is going to let us simply pass it through to the next step, but still allow us to run that section of the chain.\n",
        "3. We finally collect our response by chaining our prompt, which expects both a `\"question\"` and `\"context\"`, into our `llm`. We also, collect the `\"context\"` again so we can output it in the final response object.\n",
        "\n",
        "The key thing to keep in mind here is that we need to pass our context through *after* we've retrieved it - to populate the object in a way that doesn't require us to call it or try and use it for something else."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'loc': 'https://blog.langchain.dev/langchain-v0-1-0/', 'lastmod': '2024-02-09T21:45:58.000Z', '_id': 'b5d2495936e54a1a9b64c0f283c8b320', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}, page_content='streaming, covered later in this post.The components for LCEL are in langchain-core. We‚Äôve started to create higher level entry points for specific chains in LangChain. These will gradually replace pre-existing (now ‚ÄúLegacy‚Äù) chains, because chains built with LCEL will get streaming, ease of customization, observability, batching, retries out-of-the-box. Our goal is to make this transition seamless. Previously you may have done:ConversationalRetrievalChain.from_llm(llm, ‚Ä¶)We want to simply make'),\n",
              " Document(metadata={'source': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'loc': 'https://blog.langchain.dev/code-execution-with-langgraph/', 'lastmod': '2024-03-01T21:52:16.000Z', '_id': '22eb6ef522114775a75fadb6d4bd256a', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}, page_content='Expression Language (LCEL), which is both  bounded (~60k token) and a topic of high interest. We mined 30 days of chat-langchain for LCEL related questions (code here). We filtered for those that mentioned LCEL, from >60k chats to ~500. We clustered the ~500 and had an LLM (GPT-4, 128k) summarize clusters to give us representative questions in each. We manually reviewed and generated a ground truth answer for each question (eval set of 20 questions here). We added this dataset to'),\n",
              " Document(metadata={'source': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'loc': 'https://blog.langchain.dev/langchain-state-of-ai-2023/', 'lastmod': '2023-12-21T19:21:35.000Z', '_id': 'f0c028bc528e491cb0289747d78cb1e9', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}, page_content='UsageOne of the big additions to LangChain over the past months has been LangChain Expression Language (or LCEL for short). This is an easy way to compose components together, making it perfect for creating complex, customized chains. It is still super early on in this whole GenAI journey, and everyone is trying to figure out how exactly to make LLMs work for them. This involves a lot of experimentation and customization. LCEL makes this easy - and we saw its usage rapidly increase over the'),\n",
              " Document(metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'loc': 'https://blog.langchain.dev/langchain-v0-1-0/', 'lastmod': '2024-02-09T21:45:58.000Z', '_id': '73aa8a8970f545a195a009e577fe210f', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}, page_content='it:create_conversational_retrieval_chain(llm, ‚Ä¶)Under the hood, it will create a specific LCEL chain and return it. If you want to modify the logic - no problem! Because it‚Äôs all written in LCEL it‚Äôs easy to modify part of it without having to subclass anything or override any methods.There are a lot of chains in LangChain, and a lot of them are heavily used. We will not deprecate the legacy version of the chain until an alternative constructor function exists and has been used and')]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# üòÄ My own cell\n",
        "from operator import itemgetter\n",
        "\n",
        "qdrant_retriever.invoke(\"Why should we use LCEL?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VnGthXpzzo-R"
      },
      "outputs": [],
      "source": [
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è This is the way we implement our RAG chain using LCEL, langchain expression language, which is\n",
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è very intuitive and convient way of cunstructing a chain.\n",
        "\n",
        "\n",
        "from operator import itemgetter\n",
        "# from langchain.schema.output_parser import StrOutputParser\n",
        "# from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# üòÄ To make it easier to remember, I will import the classes this way\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    \n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    \n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": base_rag_prompt | base_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0KAPrtFMtRd"
      },
      "source": [
        "Let's get a visual understanding of our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceq3JfVLM74-",
        "outputId": "4235f7af-4430-4c08-d94d-99e482ef30af"
      },
      "outputs": [],
      "source": [
        "!pip install -qU grandalf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ocIXNGMsue",
        "outputId": "eb34fc03-7052-4825-82e1-85489ac84e78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          +---------------------------------+      \n",
            "          | Parallel<context,question>Input |      \n",
            "          +---------------------------------+      \n",
            "                    **            **               \n",
            "                  **                **             \n",
            "                **                    **           \n",
            "         +--------+                     **         \n",
            "         | Lambda |                      *         \n",
            "         +--------+                      *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "              *                          *         \n",
            "  +----------------------+          +--------+     \n",
            "  | VectorStoreRetriever |          | Lambda |     \n",
            "  +----------------------+          +--------+     \n",
            "                    **            **               \n",
            "                      **        **                 \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<context,question>Output |     \n",
            "          +----------------------------------+     \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "              +------------------------+           \n",
            "              | Parallel<context>Input |           \n",
            "              +------------------------+           \n",
            "                     ***        ***                \n",
            "                    *              *               \n",
            "                  **                **             \n",
            "           +--------+          +-------------+     \n",
            "           | Lambda |          | Passthrough |     \n",
            "           +--------+          +-------------+     \n",
            "                     ***        ***                \n",
            "                        *      *                   \n",
            "                         **  **                    \n",
            "              +-------------------------+          \n",
            "              | Parallel<context>Output |          \n",
            "              +-------------------------+          \n",
            "                            *                      \n",
            "                            *                      \n",
            "                            *                      \n",
            "          +---------------------------------+      \n",
            "          | Parallel<response,context>Input |      \n",
            "          +---------------------------------+      \n",
            "                   **              ***             \n",
            "                ***                   **           \n",
            "              **                        ***        \n",
            "+--------------------+                     **      \n",
            "| ChatPromptTemplate |                      *      \n",
            "+--------------------+                      *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "           *                                *      \n",
            "    +------------+                     +--------+  \n",
            "    | ChatOpenAI |                     | Lambda |  \n",
            "    +------------+*                  **+--------+  \n",
            "                   **              **              \n",
            "                     ***        ***                \n",
            "                        **    **                   \n",
            "          +----------------------------------+     \n",
            "          | Parallel<response,context>Output |     \n",
            "          +----------------------------------+     \n"
          ]
        }
      ],
      "source": [
        "print(retrieval_augmented_qa_chain.get_graph().draw_ascii())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQVzN_eX1M2"
      },
      "source": [
        "Let's try another visual representation:\n",
        "\n",
        "![image](https://i.imgur.com/Ad31AhL.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0C5CFRHOxtB"
      },
      "source": [
        "Let's test our chain out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "JSDyVefDaue4"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What's new in LangChain v0.2?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "yfEAoG3HLC3J",
        "outputId": "e1633e4f-5405-46c3-f344-ffc324ac69d8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LangChain v0.2 introduces several significant improvements and features, building upon the foundation laid in v0.1. Here are the key updates:\\n\\n1. **Separation of Packages**: There is now a full separation between the `langchain` and `langchain-community` packages, enhancing modularity and organization.\\n\\n2. **New Documentation**: The release includes versioned documentation, making it easier for users to navigate and reference.\\n\\n3. **Mature Agent Framework**: The agent framework has been improved for better maturity and control, allowing for more effective use of agents within the LangChain ecosystem.\\n\\n4. **Standardized LLM Interface**: There is improved standardization around the LLM (Large Language Model) interface, particularly regarding tool calling, which enhances overall usability.\\n\\n5. **Streaming Support**: The new version adds support for streaming, which can enhance responsiveness and performance in various applications.\\n\\n6. **New Partner Packages**: More than 30 new partner packages have been introduced, expanding the capabilities and integrations available to users.\\n\\nThese updates reflect a commitment to stability and usability, addressing community feedback and enhancing the overall LangChain experience. The full release of v0.2 is expected in the coming weeks.'"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': AIMessage(content='LangChain v0.2 brings several improvements, including:\\n\\n1. Full separation of the langchain package from langchain-community.\\n2. New (and versioned) documentation.\\n3. A more mature and controllable agent framework.\\n4. Improved LLM interface standardization, particularly around tool calling.\\n5. Streaming support.\\n6. Over 30 new partner packages.\\n\\nThis release builds upon the foundation laid in v0.1 and incorporates community feedback.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 1117, 'total_tokens': 1209}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-db681569-701b-4ced-9bba-4d5e978b5ef0-0', usage_metadata={'input_tokens': 1117, 'output_tokens': 92, 'total_tokens': 1209}),\n",
              " 'context': [Document(metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': '5253c8094d31419fa6a2f81344b08c4f', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}, page_content='Four months ago, we released the first stable version of LangChain. Today, we are following up by announcing a pre-release of langchain v0.2.This release builds upon the foundation laid in v0.1 and incorporates community feedback. We‚Äôre excited to share that v0.2 brings:\\xa0A much-desired full separation of langchain and langchain-community\\xa0New (and versioned!) docs\\xa0A more mature and controllable agent framework\\xa0Improved LLM interface standardization, particularly around tool callingBetter'),\n",
              "  Document(metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': '3be47673beda47f490e970d2bf5a36fe', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}, page_content=\"LangChain v0.2: A Leap Towards Stability\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain v0.2: A Leap Towards Stability\\nToday, we're announcing the pre-release of LangChain v0.2, which improves the stability and security of LangChain.\\n\\n5 min read\\nMay 10, 2024\"),\n",
              "  Document(metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'loc': 'https://blog.langchain.dev/langchain-v0-1-0/', 'lastmod': '2024-02-09T21:45:58.000Z', '_id': '6d34ecde2004455ab4f13aa60e99bda6', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}, page_content='LangChain v0.1.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAll Posts\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain v0.1.0\\n\\nBy LangChain\\n10 min read\\nJan 8, 2024'),\n",
              "  Document(metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': '0f6b5cb7a54b47dda7ebe5481d2232a3', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}, page_content='streaming support30+ new partner packages.This is just a pre-release, with the full v0.2 release coming in a few weeks. Let‚Äôs dive into what langchain v0.2 will include.Embracing stability: The evolution of LangChain architectureOne of the most notable changes in langchain v0.2 is the decoupling of the langchain package from langchain-community. As a result, langchain-community now depends on langchain-core and langchain. This is a continuation of the work we began with langchain v0.1.0 to')]}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "FSZFdCM5LFoq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context:\n",
            "page_content='Four months ago, we released the first stable version of LangChain. Today, we are following up by announcing a pre-release of langchain v0.2.This release builds upon the foundation laid in v0.1 and incorporates community feedback. We‚Äôre excited to share that v0.2 brings:¬†A much-desired full separation of langchain and langchain-community¬†New (and versioned!) docs¬†A more mature and controllable agent framework¬†Improved LLM interface standardization, particularly around tool callingBetter' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': '5253c8094d31419fa6a2f81344b08c4f', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}\n",
            "----\n",
            "Context:\n",
            "page_content='LangChain v0.2: A Leap Towards Stability\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain v0.2: A Leap Towards Stability\n",
            "Today, we're announcing the pre-release of LangChain v0.2, which improves the stability and security of LangChain.\n",
            "\n",
            "5 min read\n",
            "May 10, 2024' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': '3be47673beda47f490e970d2bf5a36fe', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}\n",
            "----\n",
            "Context:\n",
            "page_content='LangChain v0.1.0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to content\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Case Studies\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "In the Loop\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Docs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Changelog\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sign in\n",
            "Subscribe\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain v0.1.0\n",
            "\n",
            "By LangChain\n",
            "10 min read\n",
            "Jan 8, 2024' metadata={'source': 'https://blog.langchain.dev/langchain-v0-1-0/', 'loc': 'https://blog.langchain.dev/langchain-v0-1-0/', 'lastmod': '2024-02-09T21:45:58.000Z', '_id': '6d34ecde2004455ab4f13aa60e99bda6', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}\n",
            "----\n",
            "Context:\n",
            "page_content='streaming support30+ new partner packages.This is just a pre-release, with the full v0.2 release coming in a few weeks. Let‚Äôs dive into what langchain v0.2 will include.Embracing stability: The evolution of LangChain architectureOne of the most notable changes in langchain v0.2 is the decoupling of the langchain package from langchain-community. As a result, langchain-community now depends on langchain-core and langchain. This is a continuation of the work we began with langchain v0.1.0 to' metadata={'source': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'loc': 'https://blog.langchain.dev/langchain-v02-leap-to-stability/', 'lastmod': '2024-05-16T22:26:07.000Z', '_id': '0f6b5cb7a54b47dda7ebe5481d2232a3', '_collection_name': '2279a7153971439bbb7bf2af0e0a5560'}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "for context in response[\"context\"]:\n",
        "  print(\"Context:\")\n",
        "  print(context)\n",
        "  print(\"----\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nagiJ6l6noPL"
      },
      "source": [
        "Let's see if it can handle a query that is totally unrelated to the source documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "HOd2nJKZnsty"
      },
      "outputs": [],
      "source": [
        "response = retrieval_augmented_qa_chain.invoke({\"question\" : \"What is the airspeed velocity of an unladen swallow?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "TmLCKNGZLTh6",
        "outputId": "1478d061-7129-4b14-c717-c99d130a9488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJtSdDsXZXam"
      },
      "source": [
        "## Task 3: Setting Up LangSmith\n",
        "\n",
        "Now that we have a chain - we're ready to get started with LangSmith!\n",
        "\n",
        "We're going to go ahead and use the following `env` variables to get our Colab notebook set up to start reporting.\n",
        "\n",
        "If all you needed was simple monitoring - this is all you would need to do!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "iqPdBXSBD4a-"
      },
      "outputs": [],
      "source": [
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è In fact, the main topic of this notebook is LangSmith.\n",
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è LangSmith is tracing and evaluation platform we can use to get very deep understanding about the data flow of our chain\n",
        "\n",
        "\n",
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith - {unique_id}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4msyKLaIr6"
      },
      "source": [
        "### LangSmith API\n",
        "\n",
        "In order to use LangSmith - you will need a beta key, you can join the queue through the `Beta Sign Up` button on LangSmith's homepage!\n",
        "\n",
        "Join [here](https://www.langchain.com/langsmith)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVq1EYngEMhV",
        "outputId": "5560a787-8114-4c72-b95a-6a776043427a"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qy0MMBLacXv"
      },
      "source": [
        "Let's test our our first generation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eoqBtBQERXP",
        "outputId": "fc1ea857-6784-4e7e-9fd4-94fbdd9f693a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LangSmith is a framework built on the shoulders of LangChain, designed to track the performance and improve the observability of AI-powered products, particularly those utilizing large language models (LLMs). It offers features such as a sleek user interface and an SDK that provides fine-grain controls and customizability for developers.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 927, 'total_tokens': 990}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_48196bc67a', 'finish_reason': 'stop', 'logprobs': None}, id='run-51ce8ab4-a099-4257-b97d-5aac8e209eae-0', usage_metadata={'input_tokens': 927, 'output_tokens': 63, 'total_tokens': 990})"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è Let's invoke our chain and get the answer\n",
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è Thanks to Langsmith, we can get walk through every step from the very begining of our chain to the final response.\n",
        "\n",
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is LangSmith?\"}, {\"tags\" : [\"Demo Run\"]})['response']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxABFzPr2ny"
      },
      "source": [
        "## Task 4: Examining the Trace in LangSmith!\n",
        "\n",
        "Head on over to your LangSmith web UI to check out how the trace looks in LangSmith!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52o58AfsLK6"
      },
      "source": [
        "#### üèóÔ∏è Activity #1:\n",
        "\n",
        "Include a screenshot of your trace and explain what it means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxh0-thanXt"
      },
      "source": [
        "## Task 5: Loading Our Testing Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è Not only is Langsmith used for tracing, It is also used for evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsJ8uCbT7S1Z",
        "outputId": "cdcbe030-aae2-46da-8cb7-30811da9f87d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DataRepository'...\n",
            "remote: Enumerating objects: 84, done.\u001b[K\n",
            "remote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 84 (delta 23), reused 28 (delta 8), pack-reused 8 (from 1)\u001b[K\n",
            "Receiving objects: 100% (84/84), 70.08 MiB | 14.60 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/AI-Maker-Space/DataRepository.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5fmy5Gy7X03"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"langchain_blog_test_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'question': 'How did Podium improve their agent F1 response quality and reduce engineering intervention by 90%?',\n",
              "  'answer': \"Podium optimized agent behavior and reduced engineering intervention by 90% by using LangSmith for dataset curation and finetuning, which improved the agent's F1 response quality to 98%.\"},\n",
              " {'question': 'How did Athena Intelligence utilize LangSmith in their workflow to enhance the generation of complex research reports?',\n",
              "  'answer': 'Athena Intelligence used the LangSmith playground and debugging features to quickly identify LLM issues and generate complex research reports.'},\n",
              " {'question': 'What are the four strategies supported by LangGraph Cloud for handling additional context when dealing with double-texting in currently-running threads?',\n",
              "  'answer': 'LangGraph Cloud provides four different strategies for handling additional user inputs on currently-running threads of the graph. What are these strategies?\\n\\nReject, queue, interrupt, and rollback.'},\n",
              " {'question': 'What action is required after receiving the success message for confirming your subscription to the LangChain newsletter?',\n",
              "  'answer': 'Please check your inbox and click the link to confirm your subscription.'},\n",
              " {'question': 'What are the key features of the Open Source Extraction Service announced earlier this month?',\n",
              "  'answer': 'The Open Source Extraction Service is a newly launched application that extracts structured data from unstructured sources such as text and PDF documents. It is free to use but not intended for production workloads or sensitive data. The service supports various file types and allows defining and sharing extractors with custom schemas and instructions. It facilitates experimentation with the latest tools for data extraction and can be integrated into LangChain workflows.'},\n",
              " {'question': \"What are the main subsections of G√∂del's mathematical work as outlined in the context provided?\",\n",
              "  'answer': \"The question isn't provided, but based on the context, here is a potential question and its answer:\\n\\nQuestion: What are some key areas of Kurt G√∂del‚Äôs mathematical work?\\n\\nAnswer: Key areas of Kurt G√∂del's mathematical work include the Completeness Theorem, the Incompleteness Theorems, Speed-up Theorems, and his work in set theory, particularly the consistency of the Continuum Hypothesis and the Axiom of Choice with the axioms of Zermelo-Fraenkel Set Theory.\"},\n",
              " {'question': 'What should you do after receiving the \"Success!\" message when subscribing to the LangChain newsletter?',\n",
              "  'answer': 'Subscribe'},\n",
              " {'question': 'What is the role of the ISUSE token in the Self-RAG process, and what are the possible outputs it can generate?',\n",
              "  'answer': 'The ISUSE token decides whether the generation from each chunk in D is a useful response to x. The input is x, y for d in D. Output is {5, 4, 3, 2, 1}.'},\n",
              " {'question': 'What should enterprises do if they are looking to deploy OpenGPTs internally?',\n",
              "  'answer': 'gtm@langchain.dev'},\n",
              " {'question': 'What is the purpose of using LangChain in the creation of the \"Year in code\" personalized videos?',\n",
              "  'answer': \"Year in code is a personalized, AI-generated video project developed by Rubric, leveraging LangChain, OpenAI GPT-4-turbo, and several other technologies to celebrate GitHub users' contributions in 2023.\"},\n",
              " {'question': 'Explain the difference between \"pulling\" and \"pushing\" context in agent cognitive architectures, using the example of an agent interacting with a SQL database.',\n",
              "  'answer': 'First, many are actually NOT this ‚Äúagent‚Äù cognitive architecture, but rather either elaborate and complex chains, or more similar to ‚Äústate machines‚Äù. Two great public examples of this are GPT-Researcher and Sweep.dev.'},\n",
              " {'question': 'What file format needs to be migrated according to the context provided, and to what should it be migrated?',\n",
              "  'answer': 'The acceptance-test-config.yml file is in a legacy format. Please migrate to the latest format.'},\n",
              " {'question': 'What are the two required inputs for an OpenAI functions agent as mentioned in the provided context?',\n",
              "  'answer': 'The purpose of the guide \"How to design an Agent for Production\" is to explain the underlying technology and logic used to deploy a scheduling agent, Cal.ai, in production using LangChain.'},\n",
              " {'question': 'What is the name of the AI-powered developer assistant created by Robocorp to help developers write better Python automation code faster?',\n",
              "  'answer': 'ReMark helps practitioners by generating functional code snippets relevant to their use case.'},\n",
              " {'question': 'What is one of the main advantages of using OpaquePrompts for anonymizing data with LangChain?',\n",
              "  'answer': 'Option #1: do not save inputs or outputs\\n\\nIf we want to make sure we are not saving any PII data to LangSmith, we can just hide the inputs and outputs of all of our queries from LangSmith. This can be done with a few environmental variables that LangSmith will use to understand whether it needs to log inputs/outputs or not:\\n\\nLANGCHAIN_HIDE_INPUTS=true\\nLANGCHAIN_HIDE_OUTPUTS=true\\n\\nIn this way we can directly control what is logged and what is not. For more information on this, please refer to the docs. It is important to note that if using this functionality, we should consider hiding both the inputs and the outputs since the LLM might mention PII data points in its answer (‚ÄòHi John Smith! Nice to meet you‚Äô).'},\n",
              " {'question': 'What specific business requirements does Eden AI address, according to the context provided?',\n",
              "  'answer': 'Question: What are the main advantages of integrating Eden AI with LangChain?\\n\\nAnswer: The main advantages of integrating Eden AI with LangChain include:\\n1. A unified platform to access multiple LLMs and Embeddings with just one API key and minimal code.\\n2. A robust dashboard for optimizing AI investments, including monitoring usage, resource allocation, cost management, logging for debugging, and API caching.\\n3. Advanced AI capabilities, such as Explicit Content Detection, Invoice and ID parsing, Object Detection, Text-to-Speech, and Speech-to-Text, to enhance AI-powered applications.'},\n",
              " {'question': 'What role does the RecordManager play in the LangChain indexing process when syncing data sources to a vector store?',\n",
              "  'answer': 'The LangChain Indexing API helps avoid writing duplicated content into the vector store by computing hashes for each document and storing the document hash, write time, and source id in the record manager. This ensures that unchanged content is not re-written or re-embedded.'},\n",
              " {'question': 'What are the two concrete steps mentioned in the context that LangChainHub is taking to make it easy and straightforward for people to use the collection of prompts, agents, and chains?',\n",
              "  'answer': 'The LangChainHub is a platform designed to make it easy to find and share commonly used prompts, chains, and agents for building applications using the LangChain framework.'},\n",
              " {'question': 'Explain how the previous implementation of run-scoped custom callbacks in TypeScript was tedious and how the new changes improve this process. Use the provided example for reference.',\n",
              "  'answer': 'The improvements to the callbacks system in LangChain include support for concurrent runs with independent callbacks, tracing of deeply nested trees of LangChain components, and callback handlers scoped to a single request.'},\n",
              " {'question': \"What are the two main benefits provided by the integration of GPT Researcher with LangChain's OpenAI adapter, as described in the context?\",\n",
              "  'answer': 'GPT Researcher generates research reports by utilizing \"planner\" and \"execution\" agents that generate research questions, scrape online resources, summarize relevant information, and aggregate it into a final report.'},\n",
              " {'question': 'What action should you take after receiving a \"Success!\" message when subscribing to the newsletter?',\n",
              "  'answer': '\"Success! Please check your inbox and click the link to confirm your subscription.\"'},\n",
              " {'question': 'What recent measures have been taken to address the challenges of creating and maintaining high-value language model applications in production settings according to the provided context?',\n",
              "  'answer': 'LangChain will continue to add features and provide offerings that will make it easy to prototype applications quickly and bridge the gap between prototyping and putting something into production.'},\n",
              " {'question': 'What are the two key facets of the MVP browser experience where LangChain was incorporated, according to the provided context?',\n",
              "  'answer': 'The authors of \"Origin\" are currently pursuing dual degrees in which program at UC Berkeley?\\n\\nThe Management, Entrepreneurship, and Technology (M.E.T.) Program.'}]"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# üòÄ My own cell\n",
        "\n",
        "questions_and_answers = []\n",
        "\n",
        "for triplet in test_df.iterrows():\n",
        "    questions_and_answers.append({'question':triplet[1]['question'], 'answer':triplet[1]['answer']})\n",
        "\n",
        "questions_and_answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyBtPs7p9Mbz"
      },
      "source": [
        "Now we can set up our LangSmith client - and we'll add the above created dataset to our LangSmith instance!\n",
        "\n",
        "> NOTE: Read more about this process [here](https://docs.smith.langchain.com/old/evaluation/faq/manage-datasets#create-from-list-of-values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "T9exE2e6F3gF"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"langsmith-demo-dataset-aie4-triples-v3\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name, description=\"LangChain Blog Test Questions\"\n",
        ")\n",
        "\n",
        "for triplet in test_df.iterrows():\n",
        "  triplet = triplet[1]\n",
        "  client.create_example(\n",
        "      inputs={\"question\" : triplet[\"question\"], \"context\": triplet[\"context\"]},\n",
        "      outputs={\"answer\" : triplet[\"answer\"]},\n",
        "      dataset_id=dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgi14vSbFIc"
      },
      "source": [
        "## Task 6: Evaluation\n",
        "\n",
        "Now we can run the evaluation!\n",
        "\n",
        "We'll need to start by preparing some custom data preparation functions to ensure our chain works with the expected inputs/outputs from the `evaluate` process in LangSmith.\n",
        "\n",
        "> NOTE: More reading on this available [here](https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_llm_application#evaluate-a-langchain-runnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "fbjnv3bMwQKg"
      },
      "outputs": [],
      "source": [
        "def prepare_data_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.outputs[\"answer\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_data_noref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }\n",
        "\n",
        "def prepare_context_ref(run, example):\n",
        "  return {\n",
        "      \"prediction\" : run.outputs[\"response\"],\n",
        "      \"reference\" : example.inputs[\"context\"],\n",
        "      \"input\" : example.inputs[\"question\"]\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuwnMdtl9nwz"
      },
      "source": [
        "We'll be using a few custom evaluators to evaluate our pipeline, as well as a few \"built in\" methods!\n",
        "\n",
        "Check out the built-ins [here](https://docs.smith.langchain.com/reference/sdk_reference/langchain_evaluators)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è It provides many built-in evaluaotors, such as Cot, chain-of-thouht evaluator.\n",
        "#  It is also possible to define our own evaluation criterion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "501c5e204387460e8e9a585c9b2ca834",
            "323a077888c84bdfbac393d1ebef9d6c",
            "710fe93f3241401bafa10a1e0a1bda02",
            "c65a6921701742da9b67c591b19b6336",
            "17d4193e78a14050a11c5f8306b3ba0b",
            "99751d16888640078ae4820cacab5760",
            "a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "54e7ccad2e774261a3313316a1397f66",
            "41e287b8dc674f839b6485c9606207d5",
            "f7457f38948d472da4027ba1566b47e4",
            "af61b863014d4a8c960570de31a02b3f"
          ]
        },
        "id": "CENtd4K_IQa3",
        "outputId": "872581e4-9c21-414f-d25e-9454c47c0587"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\", prepare_data=prepare_context_ref)\n",
        "\n",
        "unlabeled_dopeness_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\" : {\n",
        "            \"dopeness\" : \"Is the answer to the question dope, meaning cool - awesome - and legit?\"\n",
        "        }\n",
        "    },\n",
        "    prepare_data=prepare_data_noref\n",
        ")\n",
        "\n",
        "labeled_score_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_score_string\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"accuracy\": \"Is the generated answer the same as the reference answer?\"\n",
        "        },\n",
        "    },\n",
        "    prepare_data=prepare_data_ref\n",
        ")\n",
        "\n",
        "base_rag_results = evaluate(\n",
        "    retrieval_augmented_qa_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        cot_qa_evaluator,\n",
        "        unlabeled_dopeness_evaluator,\n",
        "        labeled_score_evaluator,\n",
        "        ],\n",
        "    experiment_prefix=\"Base RAG Evaluation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è Evaluation results are automatically tracked so that we can easily check them out.\n",
        "\n",
        "# üö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏èüö∂‚Äç‚ôÄÔ∏è OK, thats gonna do it for today's video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üòÄüòÄüòÄ Screenshots of my LangSmith traces\n",
        "\n",
        "\n",
        "![Screenshot 1](Screenshots/Screenshot_1.png)\n",
        "![Screenshot 2](Screenshots/Screenshot_2.png)\n",
        "![Screenshot 3](Screenshots/Screenshot_3.png)\n",
        "![Screenshot 4](Screenshots/Screenshot_4.png)\n",
        "![Screenshot 5](Screenshots/Screenshot_5.png)\n",
        "![Screenshot 6](Screenshots/Screenshot_6.png)\n",
        "![Screenshot 7](Screenshots/Screenshot_7.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPocfrNFiYWi"
      },
      "source": [
        "#### ‚ùìQuestion #1:\n",
        "\n",
        "What conclusions can you draw about the above results?\n",
        "\n",
        "Describe in your own words what the metrics are expressing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üòÄüòÄüòÄ Answers to Question #1\n",
        "\n",
        "- CoT (Chain-of-thougt) metric\n",
        "    - It requires question, context and answer for judgement. Ground truth is not required.\n",
        "    - It focuses on whether the answer is derived from the given context in a step-by-step manner.\n",
        "    - It is sort of a binary classifier.\n",
        "\n",
        "- Dopeness metric\n",
        "    - It makes its conclusion only based on question and answer. Context and ground truth are not required.\n",
        "    - It is a custom metric about dopeness (coolness).\n",
        "    - As the evaluater pointed out in the result, this metric can be highly subjective and need for caution. \n",
        "    - It is sort of a binary classifier.\n",
        "\n",
        "- Accuracy metric\n",
        "    - It draws a conclution by question, answer and ground truth. Context is nore required.\n",
        "    - Basically it examines how close it is our chains response to ground truth.\n",
        "    - It is a 10-class claasifier."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17d4193e78a14050a11c5f8306b3ba0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "323a077888c84bdfbac393d1ebef9d6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99751d16888640078ae4820cacab5760",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a8f5f4beaa8f4a3b94002c5ed122c2a7",
            "value": ""
          }
        },
        "41e287b8dc674f839b6485c9606207d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "501c5e204387460e8e9a585c9b2ca834": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_323a077888c84bdfbac393d1ebef9d6c",
              "IPY_MODEL_710fe93f3241401bafa10a1e0a1bda02",
              "IPY_MODEL_c65a6921701742da9b67c591b19b6336"
            ],
            "layout": "IPY_MODEL_17d4193e78a14050a11c5f8306b3ba0b"
          }
        },
        "54e7ccad2e774261a3313316a1397f66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "710fe93f3241401bafa10a1e0a1bda02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e7ccad2e774261a3313316a1397f66",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41e287b8dc674f839b6485c9606207d5",
            "value": 1
          }
        },
        "99751d16888640078ae4820cacab5760": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f5f4beaa8f4a3b94002c5ed122c2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af61b863014d4a8c960570de31a02b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c65a6921701742da9b67c591b19b6336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7457f38948d472da4027ba1566b47e4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_af61b863014d4a8c960570de31a02b3f",
            "value": "‚Äá23/?‚Äá[02:10&lt;00:00,‚Äá‚Äá5.36s/it]"
          }
        },
        "f7457f38948d472da4027ba1566b47e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
